{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:22.654813Z",
     "iopub.status.busy": "2021-02-17T05:22:22.653936Z",
     "iopub.status.idle": "2021-02-17T05:22:30.860384Z",
     "shell.execute_reply": "2021-02-17T05:22:30.859095Z"
    },
    "papermill": {
     "duration": 8.228239,
     "end_time": "2021-02-17T05:22:30.860531",
     "exception": false,
     "start_time": "2021-02-17T05:22:22.632292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_addons.optimizers import Lookahead, RectifiedAdam\n",
    "from tqdm import tqdm\n",
    "from tensorflow import keras\n",
    "from albumentations import (Compose, Transpose, HorizontalFlip,\n",
    "                            VerticalFlip, RandomResizedCrop, RandomRotate90, RandomBrightnessContrast,HueSaturationValue )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:30.887646Z",
     "iopub.status.busy": "2021-02-17T05:22:30.886592Z",
     "iopub.status.idle": "2021-02-17T05:22:30.889217Z",
     "shell.execute_reply": "2021-02-17T05:22:30.889807Z"
    },
    "papermill": {
     "duration": 0.018455,
     "end_time": "2021-02-17T05:22:30.889951",
     "exception": false,
     "start_time": "2021-02-17T05:22:30.871496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "params={\n",
    "    \"img_size\": 300,\n",
    "    \"seed\": 2021\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:30.918872Z",
     "iopub.status.busy": "2021-02-17T05:22:30.917832Z",
     "iopub.status.idle": "2021-02-17T05:22:30.921123Z",
     "shell.execute_reply": "2021-02-17T05:22:30.920521Z"
    },
    "papermill": {
     "duration": 0.020803,
     "end_time": "2021-02-17T05:22:30.921240",
     "exception": false,
     "start_time": "2021-02-17T05:22:30.900437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    \n",
    "seed_everything(params[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:30.950422Z",
     "iopub.status.busy": "2021-02-17T05:22:30.949376Z",
     "iopub.status.idle": "2021-02-17T05:22:30.952628Z",
     "shell.execute_reply": "2021-02-17T05:22:30.952048Z"
    },
    "papermill": {
     "duration": 0.020529,
     "end_time": "2021-02-17T05:22:30.952747",
     "exception": false,
     "start_time": "2021-02-17T05:22:30.932218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resize_img(file_path, img_size):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, [img_size, img_size])\n",
    "    # img = tf.expand_dims(img, 0)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:30.990260Z",
     "iopub.status.busy": "2021-02-17T05:22:30.989214Z",
     "iopub.status.idle": "2021-02-17T05:22:30.992707Z",
     "shell.execute_reply": "2021-02-17T05:22:30.992129Z"
    },
    "papermill": {
     "duration": 0.029091,
     "end_time": "2021-02-17T05:22:30.992832",
     "exception": false,
     "start_time": "2021-02-17T05:22:30.963741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "augmentate = Compose([\n",
    "    RandomResizedCrop(params[\"img_size\"], params[\"img_size\"]),\n",
    "    Transpose(p=0.6),\n",
    "    HorizontalFlip(p=0.6),\n",
    "    VerticalFlip(p=0.6),\n",
    "    RandomRotate90(p=0.6)])\n",
    "\n",
    "class augmentate_data():\n",
    "    def __init__(self, ds, img_size):\n",
    "        self.ds = ds\n",
    "        self.input_shape=(img_size, img_size, 3)\n",
    "\n",
    "    def _aug_fn(self, image):\n",
    "        data = {\"image\": image}\n",
    "        aug_data = augmentate(**data)\n",
    "        aug_img = aug_data[\"image\"]\n",
    "\n",
    "        return aug_img\n",
    "\n",
    "    def _process_img(self, image):\n",
    "        aug_img = tf.numpy_function(\n",
    "            func=self._aug_fn, inp=[image], Tout=tf.float32)\n",
    "\n",
    "        return aug_img\n",
    "\n",
    "    def _set_shapes(self, img):\n",
    "        img.set_shape((self.input_shape))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def data_aug(self):\n",
    "        img = self._process_img(self.ds)\n",
    "        img = self._set_shapes(img)\n",
    "        \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:31.067676Z",
     "iopub.status.busy": "2021-02-17T05:22:31.030108Z",
     "iopub.status.idle": "2021-02-17T05:22:31.110052Z",
     "shell.execute_reply": "2021-02-17T05:22:31.110653Z"
    },
    "papermill": {
     "duration": 0.106991,
     "end_time": "2021-02-17T05:22:31.110827",
     "exception": false,
     "start_time": "2021-02-17T05:22:31.003836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"AdaBeliefOptimizer optimizer.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tabulate import tabulate\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "\n",
    "class AdaBeliefOptimizer(tf.keras.optimizers.Optimizer):\n",
    "    \"\"\"\n",
    "    It implements the AdaBeliefOptimizer proposed by\n",
    "    Juntang Zhuang et al. in [AdaBelief Optimizer: Adapting stepsizes by the belief\n",
    "    in observed gradients](https://arxiv.org/abs/2010.07468).\n",
    "    Contributor(s):\n",
    "        Jerry Yu [cryu854] <cryu854@gmail.com>\n",
    "    Example of usage:\n",
    "    ```python\n",
    "    from adabelief_tf import AdaBeliefOptimizer\n",
    "    opt = AdaBeliefOptimizer(lr=1e-3)\n",
    "    ```\n",
    "    Note: `amsgrad` is not described in the original paper. Use it with\n",
    "          caution.\n",
    "    AdaBeliefOptimizer is not a placement of the heuristic warmup, the settings should be\n",
    "    kept if warmup has already been employed and tuned in the baseline method.\n",
    "    You can enable warmup by setting `total_steps` and `warmup_proportion`:\n",
    "    ```python\n",
    "    opt = AdaBeliefOptimizer(\n",
    "        lr=1e-3,\n",
    "        total_steps=10000,\n",
    "        warmup_proportion=0.1,\n",
    "        min_lr=1e-5,\n",
    "    )\n",
    "    ```\n",
    "    In the above example, the learning rate will increase linearly\n",
    "    from 0 to `lr` in 1000 steps, then decrease linearly from `lr` to `min_lr`\n",
    "    in 9000 steps.\n",
    "    Lookahead, proposed by Michael R. Zhang et.al in the paper\n",
    "    [Lookahead Optimizer: k steps forward, 1 step back]\n",
    "    (https://arxiv.org/abs/1907.08610v1), can be integrated with AdaBeliefOptimizer,\n",
    "    which is announced by Less Wright and the new combined optimizer can also\n",
    "    be called \"Ranger\". The mechanism can be enabled by using the lookahead\n",
    "    wrapper. For example:\n",
    "    ```python\n",
    "    adabelief = AdaBeliefOptimizer()\n",
    "    ranger = tfa.optimizers.Lookahead(adabelief, sync_period=6, slow_step_size=0.5)\n",
    "    ```\n",
    "    Example of serialization:\n",
    "    ```python\n",
    "    optimizer = AdaBeliefOptimizer(learning_rate=lr_scheduler, weight_decay=wd_scheduler)\n",
    "    config = tf.keras.optimizers.serialize(optimizer)\n",
    "    new_optimizer = tf.keras.optimizers.deserialize(config, custom_objects={\"AdaBeliefOptimizer\": AdaBeliefOptimizer})\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-14,\n",
    "        weight_decay=0.0,\n",
    "        rectify=True,\n",
    "        amsgrad=False,\n",
    "        sma_threshold=5.0,\n",
    "        total_steps=0,\n",
    "        warmup_proportion=0.1,\n",
    "        min_lr=0.0,\n",
    "        name=\"AdaBeliefOptimizer\",\n",
    "        print_change_log = True,\n",
    "        **kwargs):\n",
    "        r\"\"\"Construct a new AdaBelief optimizer.\n",
    "        Args:\n",
    "            learning_rate: A `Tensor` or a floating point value, or a schedule\n",
    "                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`.\n",
    "                The learning rate.\n",
    "            beta_1: A float value or a constant float tensor.\n",
    "                The exponential decay rate for the 1st moment estimates.\n",
    "            beta_2: A float value or a constant float tensor.\n",
    "                The exponential decay rate for the 2nd moment estimates.\n",
    "            epsilon: A small constant for numerical stability.\n",
    "            weight_decay: A `Tensor` or a floating point value, or a schedule\n",
    "                that is a `tf.keras.optimizers.schedules.LearningRateSchedule`.\n",
    "                Weight decay for each parameter.\n",
    "            rectify: boolean. Whether to enable rectification as in RectifiedAdam\n",
    "            amsgrad: boolean. Whether to apply AMSGrad variant of this\n",
    "                algorithm from the paper \"On the Convergence of Adam and\n",
    "                beyond\".\n",
    "            sma_threshold. A float value.\n",
    "                The threshold for simple mean average.\n",
    "            total_steps: An integer. Total number of training steps.\n",
    "                Enable warmup by setting a positive value.\n",
    "            warmup_proportion: A floating point value.\n",
    "                The proportion of increasing steps.\n",
    "            min_lr: A floating point value. Minimum learning rate after warmup.\n",
    "            name: Optional name for the operations created when applying\n",
    "                gradients. Defaults to \"AdaBeliefOptimizer\".\n",
    "            **kwargs: keyword arguments. Allowed to be {`clipnorm`,\n",
    "                `clipvalue`, `lr`, `decay`}. `clipnorm` is clip gradients\n",
    "                by norm; `clipvalue` is clip gradients by value, `decay` is\n",
    "                included for backward compatibility to allow time inverse\n",
    "                decay of learning rate. `lr` is included for backward\n",
    "                compatibility, recommended to use `learning_rate` instead.\n",
    "        \"\"\"\n",
    "        super().__init__(name, **kwargs)\n",
    "\n",
    "        # ------------------------------------------------------------------------------\n",
    "        # Print modifications to default arguments\n",
    "        if print_change_log:\n",
    "            print(Fore.RED + 'Please check your arguments if you have upgraded adabelief-tf from version 0.0.1.')\n",
    "            print(Fore.RED + 'Modifications to default arguments:')\n",
    "            default_table = tabulate([\n",
    "                ['adabelief-tf=0.0.1','1e-8','Not supported','Not supported'],\n",
    "                ['>=0.1.0 (Current 0.2.1)','1e-14','supported','default: True']],\n",
    "                headers=['eps','weight_decouple','rectify'])\n",
    "            print(Fore.RED + default_table)\n",
    "\n",
    "            recommend_table = tabulate([\n",
    "                ['Recommended epsilon = 1e-7', 'Recommended epsilon = 1e-14'],\n",
    "                ],\n",
    "                headers=['SGD better than Adam (e.g. CNN for Image Classification)','Adam better than SGD (e.g. Transformer, GAN)'])\n",
    "            print(Fore.BLUE + recommend_table)\n",
    "\n",
    "            print(Fore.BLUE +'For a complete table of recommended hyperparameters, see')\n",
    "            print(Fore.BLUE + 'https://github.com/juntang-zhuang/Adabelief-Optimizer')\n",
    "\n",
    "            print(Fore.GREEN + 'You can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.')\n",
    "\n",
    "            print(Style.RESET_ALL)\n",
    "        # ------------------------------------------------------------------------------\n",
    "\n",
    "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
    "        self._set_hyper(\"beta_1\", beta_1)\n",
    "        self._set_hyper(\"beta_2\", beta_2)\n",
    "        self._set_hyper(\"decay\", self._initial_decay)\n",
    "        self._set_hyper(\"weight_decay\", weight_decay)\n",
    "        self._set_hyper(\"sma_threshold\", sma_threshold)\n",
    "        self._set_hyper(\"total_steps\", int(total_steps))\n",
    "        self._set_hyper(\"warmup_proportion\", warmup_proportion)\n",
    "        self._set_hyper(\"min_lr\", min_lr)\n",
    "        self.epsilon = epsilon or tf.keras.backend.epsilon()\n",
    "        self.amsgrad = amsgrad\n",
    "        self.rectify = rectify\n",
    "        self._has_weight_decay = weight_decay != 0.0\n",
    "        self._initial_total_steps = total_steps\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"v\")\n",
    "        if self.amsgrad:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, \"vhat\")\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        num_vars = int((len(params) - 1) / 2)\n",
    "        if len(weights) == 3 * num_vars + 1:\n",
    "            weights = weights[: len(params)]\n",
    "        super().set_weights(weights)\n",
    "\n",
    "    def _decayed_wd(self, var_dtype):\n",
    "        wd_t = self._get_hyper(\"weight_decay\", var_dtype)\n",
    "        if isinstance(wd_t, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            wd_t = tf.cast(wd_t(self.iterations), var_dtype)\n",
    "        return wd_t\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        wd_t = self._decayed_wd(var_dtype)\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = tf.math.pow(beta_1_t, local_step)\n",
    "        beta_2_power = tf.math.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper(\"total_steps\", var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper(\"warmup_proportion\", var_dtype)\n",
    "            min_lr = self._get_hyper(\"min_lr\", var_dtype)\n",
    "            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m_t = m.assign(\n",
    "            beta_1_t * m + (1.0 - beta_1_t) * grad, use_locking=self._use_locking\n",
    "        )\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v_t = v.assign(\n",
    "            beta_2_t * v + (1.0 - beta_2_t) * tf.math.square(grad - m_t) + epsilon_t,\n",
    "            use_locking=self._use_locking,\n",
    "        )\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n",
    "            v_corr_t = tf.math.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = tf.math.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = tf.math.sqrt(\n",
    "            (sma_t - 4.0)\n",
    "            / (sma_inf - 4.0)\n",
    "            * (sma_t - 2.0)\n",
    "            / (sma_inf - 2.0)\n",
    "            * sma_inf\n",
    "            / sma_t\n",
    "        )\n",
    "\n",
    "        if self.rectify:\n",
    "            sma_threshold = self._get_hyper(\"sma_threshold\", var_dtype)\n",
    "            var_t = tf.where(\n",
    "                sma_t >= sma_threshold,\n",
    "                r_t * m_corr_t / (v_corr_t + epsilon_t),\n",
    "                m_corr_t,\n",
    "            )\n",
    "        else:\n",
    "            var_t = m_corr_t / (v_corr_t + epsilon_t)\n",
    "\n",
    "        if self._has_weight_decay:\n",
    "            var_t += wd_t * var\n",
    "\n",
    "        var_update = var.assign_sub(lr_t * var_t, use_locking=self._use_locking)\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        var_dtype = var.dtype.base_dtype\n",
    "        lr_t = self._decayed_lr(var_dtype)\n",
    "        wd_t = self._decayed_wd(var_dtype)\n",
    "        beta_1_t = self._get_hyper(\"beta_1\", var_dtype)\n",
    "        beta_2_t = self._get_hyper(\"beta_2\", var_dtype)\n",
    "        epsilon_t = tf.convert_to_tensor(self.epsilon, var_dtype)\n",
    "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
    "        beta_1_power = tf.math.pow(beta_1_t, local_step)\n",
    "        beta_2_power = tf.math.pow(beta_2_t, local_step)\n",
    "\n",
    "        if self._initial_total_steps > 0:\n",
    "            total_steps = self._get_hyper(\"total_steps\", var_dtype)\n",
    "            warmup_steps = total_steps * self._get_hyper(\"warmup_proportion\", var_dtype)\n",
    "            min_lr = self._get_hyper(\"min_lr\", var_dtype)\n",
    "            decay_steps = tf.maximum(total_steps - warmup_steps, 1)\n",
    "            decay_rate = (min_lr - lr_t) / decay_steps\n",
    "            lr_t = tf.where(\n",
    "                local_step <= warmup_steps,\n",
    "                lr_t * (local_step / warmup_steps),\n",
    "                lr_t + decay_rate * tf.minimum(local_step - warmup_steps, decay_steps),\n",
    "            )\n",
    "\n",
    "        sma_inf = 2.0 / (1.0 - beta_2_t) - 1.0\n",
    "        sma_t = sma_inf - 2.0 * local_step * beta_2_power / (1.0 - beta_2_power)\n",
    "\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta_1_t)\n",
    "        m_t = m.assign(m * beta_1_t, use_locking=self._use_locking)\n",
    "        m_t = self._resource_scatter_add(m, indices, m_scaled_g_values)\n",
    "        m_corr_t = m_t / (1.0 - beta_1_power)\n",
    "\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        m_t_indices = tf.gather(m_t, indices)\n",
    "        v_scaled_g_values = tf.math.square(grad - m_t_indices) * (1 - beta_2_t)\n",
    "        v_t = v.assign(v * beta_2_t + epsilon_t, use_locking=self._use_locking)\n",
    "        v_t = self._resource_scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        if self.amsgrad:\n",
    "            vhat = self.get_slot(var, \"vhat\")\n",
    "            vhat_t = vhat.assign(tf.maximum(vhat, v_t), use_locking=self._use_locking)\n",
    "            v_corr_t = tf.math.sqrt(vhat_t / (1.0 - beta_2_power))\n",
    "        else:\n",
    "            vhat_t = None\n",
    "            v_corr_t = tf.math.sqrt(v_t / (1.0 - beta_2_power))\n",
    "\n",
    "        r_t = tf.math.sqrt(\n",
    "            (sma_t - 4.0)\n",
    "            / (sma_inf - 4.0)\n",
    "            * (sma_t - 2.0)\n",
    "            / (sma_inf - 2.0)\n",
    "            * sma_inf\n",
    "            / sma_t\n",
    "        )\n",
    "\n",
    "        if self.rectify:\n",
    "            sma_threshold = self._get_hyper(\"sma_threshold\", var_dtype)\n",
    "            var_t = tf.where(\n",
    "                sma_t >= sma_threshold,\n",
    "                r_t * m_corr_t / (v_corr_t + epsilon_t),\n",
    "                m_corr_t,\n",
    "            )\n",
    "        else:\n",
    "            var_t = m_corr_t / (v_corr_t + epsilon_t)\n",
    "\n",
    "        if self._has_weight_decay:\n",
    "            var_t += wd_t * var\n",
    "\n",
    "        var_update = self._resource_scatter_add(\n",
    "            var, indices, tf.gather(-lr_t * var_t, indices)\n",
    "        )\n",
    "\n",
    "        updates = [var_update, m_t, v_t]\n",
    "        if self.amsgrad:\n",
    "            updates.append(vhat_t)\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
    "                \"beta_1\": self._serialize_hyperparameter(\"beta_1\"),\n",
    "                \"beta_2\": self._serialize_hyperparameter(\"beta_2\"),\n",
    "                \"decay\": self._serialize_hyperparameter(\"decay\"),\n",
    "                \"weight_decay\": self._serialize_hyperparameter(\"weight_decay\"),\n",
    "                \"sma_threshold\": self._serialize_hyperparameter(\"sma_threshold\"),\n",
    "                \"epsilon\": self.epsilon,\n",
    "                \"amsgrad\": self.amsgrad,\n",
    "                \"rectify\": self.rectify,\n",
    "                \"total_steps\": self._serialize_hyperparameter(\"total_steps\"),\n",
    "                \"warmup_proportion\": self._serialize_hyperparameter(\n",
    "                    \"warmup_proportion\"\n",
    "                ),\n",
    "                \"min_lr\": self._serialize_hyperparameter(\"min_lr\"),\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:31.143328Z",
     "iopub.status.busy": "2021-02-17T05:22:31.141722Z",
     "iopub.status.idle": "2021-02-17T05:22:31.146535Z",
     "shell.execute_reply": "2021-02-17T05:22:31.147078Z"
    },
    "papermill": {
     "duration": 0.025121,
     "end_time": "2021-02-17T05:22:31.147220",
     "exception": false,
     "start_time": "2021-02-17T05:22:31.122099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load test data and trained model\\ntest_dir=\"../input/cassava-leaf-disease-classification/test_images/\"\\nmodel = tf.keras.models.load_model(\"../input/cassava-leaf-classification-5-fold-data/Lookahead_Adam_CosDecay_Fold_2.h5\")\\n\\n# Create dataframe for prediction\\ntest_df = pd.DataFrame(columns={\"image_id\"})\\ntest_df[\"image_id\"] =  os.listdir(test_dir)\\n\\n# Predict\\npreds = []\\n\\nfor image in test_df.image_id:\\n    img=resize_img(test_dir+image, params[\"img_size\"])\\n    prediction = model.predict(img)\\n    preds.append(np.argmax(prediction))\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load test data and trained model\n",
    "test_dir=\"../input/cassava-leaf-disease-classification/test_images/\"\n",
    "model = tf.keras.models.load_model(\"../input/cassava-leaf-classification-5-fold-data/Lookahead_Adam_CosDecay_Fold_2.h5\")\n",
    "\n",
    "# Create dataframe for prediction\n",
    "test_df = pd.DataFrame(columns={\"image_id\"})\n",
    "test_df[\"image_id\"] =  os.listdir(test_dir)\n",
    "\n",
    "# Predict\n",
    "preds = []\n",
    "\n",
    "for image in test_df.image_id:\n",
    "    img=resize_img(test_dir+image, params[\"img_size\"])\n",
    "    prediction = model.predict(img)\n",
    "    preds.append(np.argmax(prediction))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:31.179129Z",
     "iopub.status.busy": "2021-02-17T05:22:31.178105Z",
     "iopub.status.idle": "2021-02-17T05:22:31.182014Z",
     "shell.execute_reply": "2021-02-17T05:22:31.182643Z"
    },
    "papermill": {
     "duration": 0.023085,
     "end_time": "2021-02-17T05:22:31.182791",
     "exception": false,
     "start_time": "2021-02-17T05:22:31.159706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Load test data and trained model\\ntest_dir = \"../input/cassava-leaf-disease-classification/test_images/\"\\n\\nmodels = []\\n\\nfor i in range(5):\\n    models.append(tf.keras.models.load_model(\"../input/lookahead-adam-cosdecay-kfold/Lookahead_Adam_CosDecay_Fold_{}.h5\".format(i+1)))\\n\\n# Create dataframe for prediction\\ntest_df = pd.DataFrame(columns={\"image_id\"})\\ntest_df[\"image_id\"] =  os.listdir(test_dir)\\n\\n# Predict\\nTTA_steps = 6\\npreds = []\\n\\nfor image in test_df.image_id:\\n    img = resize_img(test_dir+image, params[\"img_size\"])\\n    TTA_preds = []\\n    for i in tqdm(range(TTA_steps)):\\n        aug_img = augmentate_data(img, params[\"img_size\"]).data_aug()\\n        aug_img = tf.expand_dims(aug_img, 0)\\n        models_pred = []\\n        for model in models:\\n            predict = model.predict(aug_img)\\n            models_pred.append(predict)\\n        prediction = np.mean(models_pred, axis=0)\\n        TTA_preds.append(prediction)\\n    pred = np.mean(TTA_preds, axis=0)\\n    preds.append(np.argmax(pred))\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load test data and trained model\n",
    "test_dir = \"../input/cassava-leaf-disease-classification/test_images/\"\n",
    "\n",
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    models.append(tf.keras.models.load_model(\"../input/lookahead-adam-cosdecay-kfold/Lookahead_Adam_CosDecay_Fold_{}.h5\".format(i+1)))\n",
    "\n",
    "# Create dataframe for prediction\n",
    "test_df = pd.DataFrame(columns={\"image_id\"})\n",
    "test_df[\"image_id\"] =  os.listdir(test_dir)\n",
    "\n",
    "# Predict\n",
    "TTA_steps = 6\n",
    "preds = []\n",
    "\n",
    "for image in test_df.image_id:\n",
    "    img = resize_img(test_dir+image, params[\"img_size\"])\n",
    "    TTA_preds = []\n",
    "    for i in tqdm(range(TTA_steps)):\n",
    "        aug_img = augmentate_data(img, params[\"img_size\"]).data_aug()\n",
    "        aug_img = tf.expand_dims(aug_img, 0)\n",
    "        models_pred = []\n",
    "        for model in models:\n",
    "            predict = model.predict(aug_img)\n",
    "            models_pred.append(predict)\n",
    "        prediction = np.mean(models_pred, axis=0)\n",
    "        TTA_preds.append(prediction)\n",
    "    pred = np.mean(TTA_preds, axis=0)\n",
    "    preds.append(np.argmax(pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:22:31.221011Z",
     "iopub.status.busy": "2021-02-17T05:22:31.220167Z",
     "iopub.status.idle": "2021-02-17T05:23:25.400728Z",
     "shell.execute_reply": "2021-02-17T05:23:25.399930Z"
    },
    "papermill": {
     "duration": 54.205668,
     "end_time": "2021-02-17T05:23:25.400902",
     "exception": false,
     "start_time": "2021-02-17T05:22:31.195234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-tf from version 0.0.1.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  -------------\n",
      "adabelief-tf=0.0.1       1e-08  Not supported      Not supported\n",
      ">=0.1.0 (Current 0.2.1)  1e-14  supported          default: True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended epsilon = 1e-7                                  Recommended epsilon = 1e-14\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:12<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load test data and trained model\n",
    "test_dir = \"../input/cassava-leaf-disease-classification/test_images/\"\n",
    "\n",
    "model_adabelief = tf.keras.models.load_model(\"../input/cassava-leaf-classification-5-fold-data/Lookahead_Adabelief_CosDecay_Fold_3.h5\", custom_objects={\"AdaBeliefOptimizer\": AdaBeliefOptimizer})\n",
    "model_adam = tf.keras.models.load_model(\"../input/lookahead-adam-cosdecay-kfold/Lookahead_Adam_CosDecay_Fold_3.h5\")\n",
    "model_radam = tf.keras.models.load_model(\"../input/lookahead-radam-cosdecay-bestfold/Lookahead_Radam_CosDecay_Fold_3.h5\")\n",
    "\n",
    "# Create dataframe for prediction\n",
    "test_df = pd.DataFrame(columns={\"image_id\"})\n",
    "test_df[\"image_id\"] =  os.listdir(test_dir)\n",
    "\n",
    "# Predict\n",
    "TTA_steps = 6\n",
    "preds = []\n",
    "\n",
    "for image in test_df.image_id:\n",
    "    img = resize_img(test_dir+image, params[\"img_size\"])\n",
    "    TTA_preds = []\n",
    "    for i in tqdm(range(TTA_steps)):\n",
    "        aug_img = augmentate_data(img, params[\"img_size\"]).data_aug()\n",
    "        aug_img = tf.expand_dims(aug_img, 0)\n",
    "        prediction_1 = model_adabelief.predict(aug_img)\n",
    "        prediction_2 = model_adam.predict(aug_img)\n",
    "        prediction_3 = model_radam.predict(aug_img)\n",
    "        prediction = (prediction_1 + prediction_2 + prediction_3) / 3\n",
    "        TTA_preds.append(prediction)\n",
    "    pred = np.mean(TTA_preds, axis=0)\n",
    "    preds.append(np.argmax(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:23:25.453235Z",
     "iopub.status.busy": "2021-02-17T05:23:25.449226Z",
     "iopub.status.idle": "2021-02-17T05:23:25.459594Z",
     "shell.execute_reply": "2021-02-17T05:23:25.459002Z"
    },
    "papermill": {
     "duration": 0.040818,
     "end_time": "2021-02-17T05:23:25.459720",
     "exception": false,
     "start_time": "2021-02-17T05:23:25.418902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2216849948.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  2216849948.jpg      4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_submission = pd.DataFrame({'image_id': test_df.image_id, 'label': preds})\n",
    "my_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-17T05:23:25.500491Z",
     "iopub.status.busy": "2021-02-17T05:23:25.499674Z",
     "iopub.status.idle": "2021-02-17T05:23:25.737648Z",
     "shell.execute_reply": "2021-02-17T05:23:25.736420Z"
    },
    "papermill": {
     "duration": 0.260535,
     "end_time": "2021-02-17T05:23:25.737787",
     "exception": false,
     "start_time": "2021-02-17T05:23:25.477252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_submission.to_csv('submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 69.96576,
   "end_time": "2021-02-17T05:23:27.506159",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-17T05:22:17.540399",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
